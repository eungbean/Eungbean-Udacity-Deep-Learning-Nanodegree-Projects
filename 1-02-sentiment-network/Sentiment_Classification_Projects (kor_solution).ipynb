{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification & How To \"Frame Problems\" for a Neural Network\n",
    "\n",
    "by Andrew Trask\n",
    "\n",
    "- **Twitter**: @iamtrask\n",
    "- **Blog**: http://iamtrask.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Should Already Know\n",
    "\n",
    "- neural networks, forward and back-propagation\n",
    "- stochastic gradient descent\n",
    "- mean squared error\n",
    "- and train/test splits\n",
    "\n",
    "### Where to Get Help if You Need it\n",
    "- Re-watch previous Udacity Lectures\n",
    "- Leverage the recommended Course Reading Material - [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning) (Check inside your classroom for a discount code)\n",
    "- Shoot me a tweet @iamtrask\n",
    "\n",
    "\n",
    "### Tutorial Outline:\n",
    "\n",
    "- Intro: The Importance of \"Framing a Problem\" (this lesson)\n",
    "\n",
    "- [Curate a Dataset](#lesson_1)\n",
    "- [Developing a \"Predictive Theory\"](#lesson_2)\n",
    "- [**PROJECT 1**: Quick Theory Validation](#project_1)\n",
    "\n",
    "\n",
    "- [Transforming Text to Numbers](#lesson_3)\n",
    "- [**PROJECT 2**: Creating the Input/Output Data](#project_2)\n",
    "\n",
    "\n",
    "- Putting it all together in a Neural Network (video only - nothing in notebook)\n",
    "- [**PROJECT 3**: Building our Neural Network](#project_3)\n",
    "\n",
    "\n",
    "- [Understanding Neural Noise](#lesson_4)\n",
    "- [**PROJECT 4**: Making Learning Faster by Reducing Noise](#project_4)\n",
    "\n",
    "\n",
    "- [Analyzing Inefficiencies in our Network](#lesson_5)\n",
    "- [**PROJECT 5**: Making our Network Train and Run Faster](#project_5)\n",
    "\n",
    "\n",
    "- [Further Noise Reduction](#lesson_6)\n",
    "- [**PROJECT 6**: Reducing Noise by Strategically Reducing the Vocabulary](#project_6)\n",
    "\n",
    "\n",
    "- [Analysis: What's going on in the weights?](#lesson_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# Lesson: Curate a Dataset<a id='lesson_1'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `reviews.txt` 데이터에는 모든 리뷰가 소문자로 preprocessing 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "bb95574b-21a0-4213-ae50-34363cf4f87f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "e0408810-c424-4ed4-afb9-1735e9ddbd0a"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Develop a Predictive Theory<a id='lesson_2'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "e67a709f-234f-4493-bae6-4fb192141ee0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt \t : \t reviews.txt\n",
      "\n",
      "NEGATIVE\t:\tthis movie is terrible but it has some good effects .  ...\n",
      "POSITIVE\t:\tadrian pasdar is excellent is this film . he makes a fascinating woman .  ...\n",
      "NEGATIVE\t:\tcomment this movie is impossible . is terrible  very improbable  bad interpretat...\n",
      "POSITIVE\t:\texcellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...\n",
      "NEGATIVE\t:\tif you haven  t seen this  it  s terrible . it is pure trash . i saw this about ...\n",
      "POSITIVE\t:\tthis schiffer guy is a real genius  the movie is of excellent quality and both e...\n"
     ]
    }
   ],
   "source": [
    "print(\"labels.txt \\t : \\t reviews.txt\\n\")\n",
    "pretty_print_review_and_label(2137)\n",
    "pretty_print_review_and_label(12816)\n",
    "pretty_print_review_and_label(6267)\n",
    "pretty_print_review_and_label(21934)\n",
    "pretty_print_review_and_label(5297)\n",
    "pretty_print_review_and_label(4998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label과 review를 보기 좋게 정렬하고 어떻게 시작해야 할지 생각해보자.\n",
    "Negative Review에는 'terrible'같은 단어가 많이 등장하고, postive review에는 'excellent'같은 단어가 많이 나온다.\n",
    "\n",
    "Review와 label사이의 correlation은 어떤 관계가 있을까?\n",
    "알파벳 하나 하나 단위로 보면 어떨까. 알파벳 'm'하나만 보면, 이 리뷰가 Positive인지 Negative인지 알 수 있을까? 'm','t',....이것만 봐서는 알 수 없다. 그럼 '단어'단위로 넣어보자. 'this','movie' 같은 단어는 감정이 없지만 'terrible', 'trash'만 들으면 부정적이라는 느낌이 오고 'excellent', 'genious'같은 단어를 보면 긍정적인 느낌이 느껴진다. 많이 등장할수록 그 느낌은 더 강해질 것 같다.\n",
    "\n",
    "그래, 단어 단위로 input data를 구성하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Quick Theory Validation<a id='project_1'></a>\n",
    "\n",
    "코드는 단어를 세는 방식이 되면 괜찮지 않을까?\n",
    "python의 [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) Class를 사용해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 3개의 `Counter` object를 만든다.\n",
    "\n",
    "각각 positive 단어, negative 단어, 중립적인 단어들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three Counter objects to store positive, negative and total counts\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 Positive 리뷰를 돌아가면서 단어 별 Count를 올려준다.\n",
    "그리고 모든 Negative 리뷰를 돌아가면서 단어 별 Count를 올려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Loop over all the words in all the reviews and increment the counts in the appropriate counter objects\n",
    "\n",
    "for index in range(len(reviews)):  \n",
    "    if labels[index] == 'POSITIVE':\n",
    "        for word in reviews[index].split(\" \"):\n",
    "            positive_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else :\n",
    "        for word in reviews[index].split(\" \"):\n",
    "            negative_counts[word] += 1\n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 count를 내림차순으로 정렬한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 550468),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732),\n",
       " ('good', 7720),\n",
       " ('more', 7521),\n",
       " ('when', 7456),\n",
       " ('some', 7441),\n",
       " ('if', 7285),\n",
       " ('just', 7152),\n",
       " ('can', 7001),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('my', 6488),\n",
       " ('great', 6419),\n",
       " ('well', 6405),\n",
       " ('up', 6321),\n",
       " ('which', 6267),\n",
       " ('their', 6107),\n",
       " ('see', 6026),\n",
       " ('also', 5550),\n",
       " ('we', 5531),\n",
       " ('really', 5476),\n",
       " ('would', 5400),\n",
       " ('will', 5218),\n",
       " ('me', 5167),\n",
       " ('had', 5148),\n",
       " ('only', 5137),\n",
       " ('him', 5018),\n",
       " ('even', 4964),\n",
       " ('most', 4864),\n",
       " ('other', 4858),\n",
       " ('were', 4782),\n",
       " ('first', 4755),\n",
       " ('than', 4736),\n",
       " ('much', 4685),\n",
       " ('its', 4622),\n",
       " ('no', 4574),\n",
       " ('into', 4544),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('get', 4272),\n",
       " ('how', 4213),\n",
       " ('life', 4199),\n",
       " ('been', 4189),\n",
       " ('because', 4079),\n",
       " ('way', 4036),\n",
       " ('do', 3941),\n",
       " ('made', 3823),\n",
       " ('films', 3813),\n",
       " ('them', 3805),\n",
       " ('after', 3800),\n",
       " ('many', 3766),\n",
       " ('two', 3733),\n",
       " ('too', 3659),\n",
       " ('think', 3655),\n",
       " ('movies', 3586),\n",
       " ('characters', 3560),\n",
       " ('character', 3514),\n",
       " ('don', 3468),\n",
       " ('man', 3460),\n",
       " ('show', 3432),\n",
       " ('watch', 3424),\n",
       " ('seen', 3414),\n",
       " ('then', 3358),\n",
       " ('little', 3341),\n",
       " ('still', 3340),\n",
       " ('make', 3303),\n",
       " ('could', 3237),\n",
       " ('never', 3226),\n",
       " ('being', 3217),\n",
       " ('where', 3173),\n",
       " ('does', 3069),\n",
       " ('over', 3017),\n",
       " ('any', 3002),\n",
       " ('while', 2899),\n",
       " ('know', 2833),\n",
       " ('did', 2790),\n",
       " ('years', 2758),\n",
       " ('here', 2740),\n",
       " ('ever', 2734),\n",
       " ('end', 2696),\n",
       " ('these', 2694),\n",
       " ('such', 2590),\n",
       " ('real', 2568),\n",
       " ('scene', 2567),\n",
       " ('back', 2547),\n",
       " ('those', 2485),\n",
       " ('though', 2475),\n",
       " ('off', 2463),\n",
       " ('new', 2458),\n",
       " ('your', 2453),\n",
       " ('go', 2440),\n",
       " ('acting', 2437),\n",
       " ('plot', 2432),\n",
       " ('world', 2429),\n",
       " ('scenes', 2427),\n",
       " ('say', 2414),\n",
       " ('through', 2409),\n",
       " ('makes', 2390),\n",
       " ('better', 2381),\n",
       " ('now', 2368),\n",
       " ('work', 2346),\n",
       " ('young', 2343),\n",
       " ('old', 2311),\n",
       " ('ve', 2307),\n",
       " ('find', 2272),\n",
       " ('both', 2248),\n",
       " ('before', 2177),\n",
       " ('us', 2162),\n",
       " ('again', 2158),\n",
       " ('series', 2153),\n",
       " ('quite', 2143),\n",
       " ('something', 2135),\n",
       " ('cast', 2133),\n",
       " ('should', 2121),\n",
       " ('part', 2098),\n",
       " ('always', 2088),\n",
       " ('lot', 2087),\n",
       " ('another', 2075),\n",
       " ('actors', 2047),\n",
       " ('director', 2040),\n",
       " ('family', 2032),\n",
       " ('between', 2016),\n",
       " ('own', 2016),\n",
       " ('m', 1998),\n",
       " ('may', 1997),\n",
       " ('same', 1972),\n",
       " ('role', 1967),\n",
       " ('watching', 1966),\n",
       " ('every', 1954),\n",
       " ('funny', 1953),\n",
       " ('doesn', 1935),\n",
       " ('performance', 1928),\n",
       " ('few', 1918),\n",
       " ('bad', 1907),\n",
       " ('look', 1900),\n",
       " ('re', 1884),\n",
       " ('why', 1855),\n",
       " ('things', 1849),\n",
       " ('times', 1832),\n",
       " ('big', 1815),\n",
       " ('however', 1795),\n",
       " ('actually', 1790),\n",
       " ('action', 1789),\n",
       " ('going', 1783),\n",
       " ('bit', 1757),\n",
       " ('comedy', 1742),\n",
       " ('down', 1740),\n",
       " ('music', 1738),\n",
       " ('must', 1728),\n",
       " ('take', 1709),\n",
       " ('saw', 1692),\n",
       " ('long', 1690),\n",
       " ('right', 1688),\n",
       " ('fun', 1686),\n",
       " ('fact', 1684),\n",
       " ('excellent', 1683),\n",
       " ('around', 1674),\n",
       " ('didn', 1672),\n",
       " ('without', 1671),\n",
       " ('thing', 1662),\n",
       " ('thought', 1639),\n",
       " ('got', 1635),\n",
       " ('each', 1630),\n",
       " ('day', 1614),\n",
       " ('feel', 1597),\n",
       " ('seems', 1596),\n",
       " ('come', 1594),\n",
       " ('done', 1586),\n",
       " ('beautiful', 1580),\n",
       " ('especially', 1572),\n",
       " ('played', 1571),\n",
       " ('almost', 1566),\n",
       " ('want', 1562),\n",
       " ('yet', 1556),\n",
       " ('give', 1553),\n",
       " ('pretty', 1549),\n",
       " ('last', 1543),\n",
       " ('since', 1519),\n",
       " ('different', 1504),\n",
       " ('although', 1501),\n",
       " ('gets', 1490),\n",
       " ('true', 1487),\n",
       " ('interesting', 1481),\n",
       " ('job', 1470),\n",
       " ('enough', 1455),\n",
       " ('our', 1454),\n",
       " ('shows', 1447),\n",
       " ('horror', 1441),\n",
       " ('woman', 1439),\n",
       " ('tv', 1400),\n",
       " ('probably', 1398),\n",
       " ('father', 1395),\n",
       " ('original', 1393),\n",
       " ('girl', 1390),\n",
       " ('point', 1379),\n",
       " ('plays', 1378),\n",
       " ('wonderful', 1372),\n",
       " ('far', 1358),\n",
       " ('course', 1358),\n",
       " ('john', 1350),\n",
       " ('rather', 1340),\n",
       " ('isn', 1328),\n",
       " ('ll', 1326),\n",
       " ('later', 1324),\n",
       " ('dvd', 1324),\n",
       " ('whole', 1310),\n",
       " ('war', 1310),\n",
       " ('d', 1307),\n",
       " ('found', 1306),\n",
       " ('away', 1306),\n",
       " ('screen', 1305),\n",
       " ('nothing', 1300),\n",
       " ('year', 1297),\n",
       " ('once', 1296),\n",
       " ('hard', 1294),\n",
       " ('together', 1280),\n",
       " ('set', 1277),\n",
       " ('am', 1277),\n",
       " ('having', 1266),\n",
       " ('making', 1265),\n",
       " ('place', 1263),\n",
       " ('might', 1260),\n",
       " ('comes', 1260),\n",
       " ('sure', 1253),\n",
       " ('american', 1248),\n",
       " ('play', 1245),\n",
       " ('kind', 1244),\n",
       " ('perfect', 1242),\n",
       " ('takes', 1242),\n",
       " ('performances', 1237),\n",
       " ('himself', 1230),\n",
       " ('worth', 1221),\n",
       " ('everyone', 1221),\n",
       " ('anyone', 1214),\n",
       " ('actor', 1203),\n",
       " ('three', 1201),\n",
       " ('wife', 1196),\n",
       " ('classic', 1192),\n",
       " ('goes', 1186),\n",
       " ('ending', 1178),\n",
       " ('version', 1168),\n",
       " ('star', 1149),\n",
       " ('enjoy', 1146),\n",
       " ('book', 1142),\n",
       " ('nice', 1132),\n",
       " ('everything', 1128),\n",
       " ('during', 1124),\n",
       " ('put', 1118),\n",
       " ('seeing', 1111),\n",
       " ('least', 1102),\n",
       " ('house', 1100),\n",
       " ('high', 1095),\n",
       " ('watched', 1094),\n",
       " ('loved', 1087),\n",
       " ('men', 1087),\n",
       " ('night', 1082),\n",
       " ('anything', 1075),\n",
       " ('believe', 1071),\n",
       " ('guy', 1071),\n",
       " ('top', 1063),\n",
       " ('amazing', 1058),\n",
       " ('hollywood', 1056),\n",
       " ('looking', 1053),\n",
       " ('main', 1044),\n",
       " ('definitely', 1043),\n",
       " ('gives', 1031),\n",
       " ('home', 1029),\n",
       " ('seem', 1028),\n",
       " ('episode', 1023),\n",
       " ('audience', 1020),\n",
       " ('sense', 1020),\n",
       " ('truly', 1017),\n",
       " ('special', 1011),\n",
       " ('second', 1009),\n",
       " ('short', 1009),\n",
       " ('fan', 1009),\n",
       " ('mind', 1005),\n",
       " ('human', 1001),\n",
       " ('recommend', 999),\n",
       " ('full', 996),\n",
       " ('black', 995),\n",
       " ('help', 991),\n",
       " ('along', 989),\n",
       " ('trying', 987),\n",
       " ('small', 986),\n",
       " ('death', 985),\n",
       " ('friends', 981),\n",
       " ('remember', 974),\n",
       " ('often', 970),\n",
       " ('said', 966),\n",
       " ('favorite', 962),\n",
       " ('heart', 959),\n",
       " ('early', 957),\n",
       " ('left', 956),\n",
       " ('until', 955),\n",
       " ('script', 954),\n",
       " ('let', 954),\n",
       " ('maybe', 937),\n",
       " ('today', 936),\n",
       " ('live', 934),\n",
       " ('less', 934),\n",
       " ('moments', 933),\n",
       " ('others', 929),\n",
       " ('brilliant', 926),\n",
       " ('shot', 925),\n",
       " ('liked', 923),\n",
       " ('become', 916),\n",
       " ('won', 915),\n",
       " ('used', 910),\n",
       " ('style', 907),\n",
       " ('mother', 895),\n",
       " ('lives', 894),\n",
       " ('came', 893),\n",
       " ('stars', 890),\n",
       " ('cinema', 889),\n",
       " ('looks', 885),\n",
       " ('perhaps', 884),\n",
       " ('read', 882),\n",
       " ('enjoyed', 879),\n",
       " ('boy', 875),\n",
       " ('drama', 873),\n",
       " ('highly', 871),\n",
       " ('given', 870),\n",
       " ('playing', 867),\n",
       " ('use', 864),\n",
       " ('next', 859),\n",
       " ('women', 858),\n",
       " ('fine', 857),\n",
       " ('effects', 856),\n",
       " ('kids', 854),\n",
       " ('entertaining', 853),\n",
       " ('need', 852),\n",
       " ('line', 850),\n",
       " ('works', 848),\n",
       " ('someone', 847),\n",
       " ('mr', 836),\n",
       " ('simply', 835),\n",
       " ('picture', 833),\n",
       " ('children', 833),\n",
       " ('face', 831),\n",
       " ('keep', 831),\n",
       " ('friend', 831),\n",
       " ('dark', 830),\n",
       " ('overall', 828),\n",
       " ('certainly', 828),\n",
       " ('minutes', 827),\n",
       " ('wasn', 824),\n",
       " ('history', 822),\n",
       " ('finally', 820),\n",
       " ('couple', 816),\n",
       " ('against', 815),\n",
       " ('son', 809),\n",
       " ('understand', 808),\n",
       " ('lost', 807),\n",
       " ('michael', 805),\n",
       " ('else', 801),\n",
       " ('throughout', 798),\n",
       " ('fans', 797),\n",
       " ('city', 792),\n",
       " ('reason', 789),\n",
       " ('written', 787),\n",
       " ('production', 787),\n",
       " ('several', 784),\n",
       " ('school', 783),\n",
       " ('based', 781),\n",
       " ('rest', 781),\n",
       " ('try', 780),\n",
       " ('dead', 776),\n",
       " ('hope', 775),\n",
       " ('strong', 768),\n",
       " ('white', 765),\n",
       " ('tell', 759),\n",
       " ('itself', 758),\n",
       " ('half', 753),\n",
       " ('person', 749),\n",
       " ('sometimes', 746),\n",
       " ('past', 744),\n",
       " ('start', 744),\n",
       " ('genre', 743),\n",
       " ('beginning', 739),\n",
       " ('final', 739),\n",
       " ('town', 738),\n",
       " ('art', 734),\n",
       " ('humor', 732),\n",
       " ('game', 732),\n",
       " ('yes', 731),\n",
       " ('idea', 731),\n",
       " ('late', 730),\n",
       " ('becomes', 729),\n",
       " ('despite', 729),\n",
       " ('able', 726),\n",
       " ('case', 726),\n",
       " ('money', 723),\n",
       " ('child', 721),\n",
       " ('completely', 721),\n",
       " ('side', 719),\n",
       " ('camera', 716),\n",
       " ('getting', 714),\n",
       " ('instead', 712),\n",
       " ('soon', 702),\n",
       " ('under', 700),\n",
       " ('viewer', 699),\n",
       " ('age', 697),\n",
       " ('days', 696),\n",
       " ('stories', 696),\n",
       " ('felt', 694),\n",
       " ('simple', 694),\n",
       " ('roles', 693),\n",
       " ('video', 688),\n",
       " ('name', 683),\n",
       " ('either', 683),\n",
       " ('doing', 677),\n",
       " ('turns', 674),\n",
       " ('wants', 671),\n",
       " ('close', 671),\n",
       " ('title', 669),\n",
       " ('wrong', 668),\n",
       " ('went', 666),\n",
       " ('james', 665),\n",
       " ('evil', 659),\n",
       " ('budget', 657),\n",
       " ('episodes', 657),\n",
       " ('relationship', 655),\n",
       " ('fantastic', 653),\n",
       " ('piece', 653),\n",
       " ('david', 651),\n",
       " ('turn', 648),\n",
       " ('murder', 646),\n",
       " ('parts', 645),\n",
       " ('brother', 644),\n",
       " ('absolutely', 643),\n",
       " ('head', 643),\n",
       " ('experience', 642),\n",
       " ('eyes', 641),\n",
       " ('sex', 638),\n",
       " ('direction', 637),\n",
       " ('called', 637),\n",
       " ('directed', 636),\n",
       " ('lines', 634),\n",
       " ('behind', 633),\n",
       " ('sort', 632),\n",
       " ('actress', 631),\n",
       " ('lead', 630),\n",
       " ('oscar', 628),\n",
       " ('including', 627),\n",
       " ('example', 627),\n",
       " ('known', 625),\n",
       " ('musical', 625),\n",
       " ('chance', 621),\n",
       " ('score', 620),\n",
       " ('already', 619),\n",
       " ('feeling', 619),\n",
       " ('hit', 619),\n",
       " ('voice', 615),\n",
       " ('moment', 612),\n",
       " ('living', 612),\n",
       " ('low', 610),\n",
       " ('supporting', 610),\n",
       " ('ago', 609),\n",
       " ('themselves', 608),\n",
       " ('reality', 605),\n",
       " ('hilarious', 605),\n",
       " ('jack', 604),\n",
       " ('told', 603),\n",
       " ('hand', 601),\n",
       " ('quality', 600),\n",
       " ('moving', 600),\n",
       " ('dialogue', 600),\n",
       " ('song', 599),\n",
       " ('happy', 599),\n",
       " ('matter', 598),\n",
       " ('paul', 598),\n",
       " ('light', 594),\n",
       " ('future', 593),\n",
       " ('entire', 592),\n",
       " ('finds', 591),\n",
       " ('gave', 589),\n",
       " ('laugh', 587),\n",
       " ('released', 586),\n",
       " ('expect', 584),\n",
       " ('fight', 581),\n",
       " ('particularly', 580),\n",
       " ('cinematography', 579),\n",
       " ('police', 579),\n",
       " ('whose', 578),\n",
       " ('type', 578),\n",
       " ('sound', 578),\n",
       " ('view', 573),\n",
       " ('enjoyable', 573),\n",
       " ('number', 572),\n",
       " ('romantic', 572),\n",
       " ('husband', 572),\n",
       " ('daughter', 572),\n",
       " ('documentary', 571),\n",
       " ('self', 570),\n",
       " ('superb', 569),\n",
       " ('modern', 569),\n",
       " ('took', 569),\n",
       " ('robert', 569),\n",
       " ('mean', 566),\n",
       " ('shown', 563),\n",
       " ('coming', 561),\n",
       " ('important', 560),\n",
       " ('king', 559),\n",
       " ('leave', 559),\n",
       " ('change', 558),\n",
       " ('somewhat', 555),\n",
       " ('wanted', 555),\n",
       " ('tells', 554),\n",
       " ('events', 552),\n",
       " ('run', 552),\n",
       " ('career', 552),\n",
       " ('country', 552),\n",
       " ('heard', 550),\n",
       " ('season', 550),\n",
       " ('greatest', 549),\n",
       " ('girls', 549),\n",
       " ('etc', 547),\n",
       " ('care', 546),\n",
       " ('starts', 545),\n",
       " ('english', 542),\n",
       " ('killer', 541),\n",
       " ('tale', 540),\n",
       " ('guys', 540),\n",
       " ('totally', 540),\n",
       " ('animation', 540),\n",
       " ('usual', 539),\n",
       " ('miss', 535),\n",
       " ('opinion', 535),\n",
       " ('easy', 531),\n",
       " ('violence', 531),\n",
       " ('songs', 530),\n",
       " ('british', 528),\n",
       " ('says', 526),\n",
       " ('realistic', 525),\n",
       " ('writing', 524),\n",
       " ('writer', 522),\n",
       " ('act', 522),\n",
       " ('comic', 521),\n",
       " ('thriller', 519),\n",
       " ('television', 517),\n",
       " ('power', 516),\n",
       " ('ones', 515),\n",
       " ('kid', 514),\n",
       " ('york', 513),\n",
       " ('novel', 513),\n",
       " ('alone', 512),\n",
       " ('problem', 512),\n",
       " ('attention', 509),\n",
       " ('involved', 508),\n",
       " ('kill', 507),\n",
       " ('extremely', 507),\n",
       " ('seemed', 506),\n",
       " ('hero', 505),\n",
       " ('french', 505),\n",
       " ('rock', 504),\n",
       " ('stuff', 501),\n",
       " ('wish', 499),\n",
       " ('begins', 498),\n",
       " ('taken', 497),\n",
       " ('sad', 497),\n",
       " ('ways', 496),\n",
       " ('richard', 495),\n",
       " ('knows', 494),\n",
       " ('atmosphere', 493),\n",
       " ('similar', 491),\n",
       " ('surprised', 491),\n",
       " ('taking', 491),\n",
       " ('car', 491),\n",
       " ('george', 490),\n",
       " ('perfectly', 490),\n",
       " ('across', 489),\n",
       " ('team', 489),\n",
       " ('eye', 489),\n",
       " ('sequence', 489),\n",
       " ('room', 488),\n",
       " ('due', 488),\n",
       " ('among', 488),\n",
       " ('serious', 488),\n",
       " ('powerful', 488),\n",
       " ('strange', 487),\n",
       " ('order', 487),\n",
       " ('cannot', 487),\n",
       " ('b', 487),\n",
       " ('beauty', 486),\n",
       " ('famous', 485),\n",
       " ('happened', 484),\n",
       " ('tries', 484),\n",
       " ('herself', 484),\n",
       " ('myself', 484),\n",
       " ('class', 483),\n",
       " ('four', 482),\n",
       " ('cool', 481),\n",
       " ('release', 479),\n",
       " ('anyway', 479),\n",
       " ('theme', 479),\n",
       " ('opening', 478),\n",
       " ('entertainment', 477),\n",
       " ('slow', 475),\n",
       " ('ends', 475),\n",
       " ('unique', 475),\n",
       " ('exactly', 475),\n",
       " ('easily', 474),\n",
       " ('level', 474),\n",
       " ('o', 474),\n",
       " ('red', 474),\n",
       " ('interest', 472),\n",
       " ('happen', 471),\n",
       " ('crime', 470),\n",
       " ('viewing', 468),\n",
       " ('sets', 467),\n",
       " ('memorable', 467),\n",
       " ('stop', 466),\n",
       " ('group', 466),\n",
       " ('problems', 463),\n",
       " ('dance', 463),\n",
       " ('working', 463),\n",
       " ('sister', 463),\n",
       " ('message', 463),\n",
       " ('knew', 462),\n",
       " ('mystery', 461),\n",
       " ('nature', 461),\n",
       " ('bring', 460),\n",
       " ('believable', 459),\n",
       " ('thinking', 459),\n",
       " ('brought', 459),\n",
       " ('mostly', 458),\n",
       " ('disney', 457),\n",
       " ('couldn', 457),\n",
       " ('society', 456),\n",
       " ('lady', 455),\n",
       " ('within', 455),\n",
       " ('blood', 454),\n",
       " ('parents', 453),\n",
       " ('upon', 453),\n",
       " ('viewers', 453),\n",
       " ('meets', 452),\n",
       " ('form', 452),\n",
       " ('peter', 452),\n",
       " ('tom', 452),\n",
       " ('usually', 452),\n",
       " ('soundtrack', 452),\n",
       " ('local', 450),\n",
       " ('certain', 448),\n",
       " ('follow', 448),\n",
       " ('whether', 447),\n",
       " ('possible', 446),\n",
       " ('emotional', 445),\n",
       " ('killed', 444),\n",
       " ('above', 444),\n",
       " ('de', 444),\n",
       " ('god', 443),\n",
       " ('middle', 443),\n",
       " ('needs', 442),\n",
       " ('happens', 442),\n",
       " ('flick', 442),\n",
       " ('masterpiece', 441),\n",
       " ('period', 440),\n",
       " ('major', 440),\n",
       " ('named', 439),\n",
       " ('haven', 439),\n",
       " ('particular', 438),\n",
       " ('th', 438),\n",
       " ('earth', 437),\n",
       " ('feature', 437),\n",
       " ('stand', 436),\n",
       " ('words', 435),\n",
       " ('typical', 435),\n",
       " ('elements', 433),\n",
       " ('obviously', 433),\n",
       " ('romance', 431),\n",
       " ('jane', 430),\n",
       " ('yourself', 427),\n",
       " ('showing', 427),\n",
       " ('brings', 426),\n",
       " ('fantasy', 426),\n",
       " ('guess', 423),\n",
       " ('america', 423),\n",
       " ('unfortunately', 422),\n",
       " ('huge', 422),\n",
       " ('indeed', 421),\n",
       " ('running', 421),\n",
       " ('talent', 420),\n",
       " ('stage', 419),\n",
       " ('started', 418),\n",
       " ('leads', 417),\n",
       " ('sweet', 417),\n",
       " ('japanese', 417),\n",
       " ('poor', 416),\n",
       " ('deal', 416),\n",
       " ('incredible', 413),\n",
       " ('personal', 413),\n",
       " ('fast', 412),\n",
       " ('became', 410),\n",
       " ('deep', 410),\n",
       " ('hours', 409),\n",
       " ('giving', 408),\n",
       " ('nearly', 408),\n",
       " ('dream', 408),\n",
       " ('clearly', 407),\n",
       " ('turned', 407),\n",
       " ('obvious', 406),\n",
       " ('near', 406),\n",
       " ('cut', 405),\n",
       " ('surprise', 405),\n",
       " ('era', 404),\n",
       " ('body', 404),\n",
       " ('hour', 403),\n",
       " ('female', 403),\n",
       " ('five', 403),\n",
       " ('note', 399),\n",
       " ('learn', 398),\n",
       " ('truth', 398),\n",
       " ('except', 397),\n",
       " ('feels', 397),\n",
       " ('match', 397),\n",
       " ('tony', 397),\n",
       " ('filmed', 394),\n",
       " ('clear', 394),\n",
       " ('complete', 394),\n",
       " ('street', 393),\n",
       " ('eventually', 393),\n",
       " ('keeps', 393),\n",
       " ('older', 393),\n",
       " ('lots', 393),\n",
       " ('buy', 392),\n",
       " ('william', 391),\n",
       " ('stewart', 391),\n",
       " ('fall', 390),\n",
       " ('joe', 390),\n",
       " ('meet', 390),\n",
       " ('unlike', 389),\n",
       " ('talking', 389),\n",
       " ('shots', 389),\n",
       " ('rating', 389),\n",
       " ('difficult', 389),\n",
       " ('dramatic', 388),\n",
       " ('means', 388),\n",
       " ('situation', 386),\n",
       " ('wonder', 386),\n",
       " ('present', 386),\n",
       " ('appears', 386),\n",
       " ('subject', 386),\n",
       " ('comments', 385),\n",
       " ('general', 383),\n",
       " ('sequences', 383),\n",
       " ('lee', 383),\n",
       " ('points', 382),\n",
       " ('earlier', 382),\n",
       " ('gone', 379),\n",
       " ('check', 379),\n",
       " ('suspense', 378),\n",
       " ('recommended', 378),\n",
       " ('ten', 378),\n",
       " ('third', 377),\n",
       " ('business', 377),\n",
       " ('talk', 375),\n",
       " ('leaves', 375),\n",
       " ('beyond', 375),\n",
       " ('portrayal', 374),\n",
       " ('beautifully', 373),\n",
       " ('single', 372),\n",
       " ('bill', 372),\n",
       " ('plenty', 371),\n",
       " ('word', 371),\n",
       " ('whom', 370),\n",
       " ('falls', 370),\n",
       " ('scary', 369),\n",
       " ('non', 369),\n",
       " ('figure', 369),\n",
       " ('battle', 369),\n",
       " ('using', 368),\n",
       " ('return', 368),\n",
       " ('doubt', 367),\n",
       " ('add', 367),\n",
       " ('hear', 366),\n",
       " ('solid', 366),\n",
       " ('success', 366),\n",
       " ('jokes', 365),\n",
       " ('oh', 365),\n",
       " ('touching', 365),\n",
       " ('political', 365),\n",
       " ('hell', 364),\n",
       " ('awesome', 364),\n",
       " ('boys', 364),\n",
       " ('sexual', 362),\n",
       " ('recently', 362),\n",
       " ('dog', 362),\n",
       " ('please', 361),\n",
       " ('wouldn', 361),\n",
       " ('straight', 361),\n",
       " ('features', 361),\n",
       " ('forget', 360),\n",
       " ('setting', 360),\n",
       " ('lack', 360),\n",
       " ('married', 359),\n",
       " ('mark', 359),\n",
       " ('social', 357),\n",
       " ('interested', 356),\n",
       " ('adventure', 356),\n",
       " ('actual', 355),\n",
       " ('terrific', 355),\n",
       " ('sees', 355),\n",
       " ('brothers', 355),\n",
       " ('move', 354),\n",
       " ('call', 354),\n",
       " ('various', 353),\n",
       " ('theater', 353),\n",
       " ('dr', 353),\n",
       " ('animated', 352),\n",
       " ('western', 351),\n",
       " ('baby', 350),\n",
       " ('space', 350),\n",
       " ('leading', 348),\n",
       " ('disappointed', 348),\n",
       " ('portrayed', 346),\n",
       " ('aren', 346),\n",
       " ('screenplay', 345),\n",
       " ('smith', 345),\n",
       " ('towards', 344),\n",
       " ('hate', 344),\n",
       " ('noir', 343),\n",
       " ('outstanding', 342),\n",
       " ('decent', 342),\n",
       " ('kelly', 342),\n",
       " ('directors', 341),\n",
       " ('journey', 341),\n",
       " ('none', 340),\n",
       " ('looked', 340),\n",
       " ('effective', 340),\n",
       " ('storyline', 339),\n",
       " ('caught', 339),\n",
       " ('sci', 339),\n",
       " ('fi', 339),\n",
       " ('cold', 339),\n",
       " ('mary', 339),\n",
       " ('rich', 338),\n",
       " ('charming', 338),\n",
       " ('popular', 337),\n",
       " ('rare', 337),\n",
       " ('manages', 337),\n",
       " ('harry', 337),\n",
       " ('spirit', 336),\n",
       " ('appreciate', 335),\n",
       " ('open', 335),\n",
       " ('moves', 334),\n",
       " ('basically', 334),\n",
       " ('acted', 334),\n",
       " ('inside', 333),\n",
       " ('boring', 333),\n",
       " ('century', 333),\n",
       " ('mention', 333),\n",
       " ('deserves', 333),\n",
       " ('subtle', 333),\n",
       " ('pace', 333),\n",
       " ('familiar', 332),\n",
       " ('background', 332),\n",
       " ('ben', 331),\n",
       " ('creepy', 330),\n",
       " ('supposed', 330),\n",
       " ('secret', 329),\n",
       " ('die', 328),\n",
       " ('jim', 328),\n",
       " ('question', 327),\n",
       " ('effect', 327),\n",
       " ('natural', 327),\n",
       " ('impressive', 326),\n",
       " ('rate', 326),\n",
       " ('language', 326),\n",
       " ('saying', 325),\n",
       " ('intelligent', 325),\n",
       " ('telling', 324),\n",
       " ('realize', 324),\n",
       " ('material', 324),\n",
       " ('scott', 324),\n",
       " ('singing', 323),\n",
       " ('dancing', 322),\n",
       " ('visual', 321),\n",
       " ('adult', 321),\n",
       " ('imagine', 321),\n",
       " ('kept', 320),\n",
       " ('office', 320),\n",
       " ('uses', 319),\n",
       " ('pure', 318),\n",
       " ('wait', 318),\n",
       " ('stunning', 318),\n",
       " ('review', 317),\n",
       " ('previous', 317),\n",
       " ('copy', 317),\n",
       " ('seriously', 317),\n",
       " ('reading', 316),\n",
       " ('create', 316),\n",
       " ('hot', 316),\n",
       " ('created', 316),\n",
       " ('magic', 316),\n",
       " ('somehow', 316),\n",
       " ('stay', 315),\n",
       " ('attempt', 315),\n",
       " ('escape', 315),\n",
       " ('crazy', 315),\n",
       " ('air', 315),\n",
       " ('frank', 315),\n",
       " ('hands', 314),\n",
       " ('filled', 313),\n",
       " ('expected', 312),\n",
       " ('average', 312),\n",
       " ('surprisingly', 312),\n",
       " ('complex', 311),\n",
       " ('quickly', 310),\n",
       " ('successful', 310),\n",
       " ('studio', 310),\n",
       " ('plus', 309),\n",
       " ('male', 309),\n",
       " ('co', 307),\n",
       " ('images', 306),\n",
       " ('casting', 306),\n",
       " ('following', 306),\n",
       " ('minute', 306),\n",
       " ('exciting', 306),\n",
       " ('members', 305),\n",
       " ('follows', 305),\n",
       " ('themes', 305),\n",
       " ('german', 305),\n",
       " ('reasons', 305),\n",
       " ('e', 305),\n",
       " ('touch', 304),\n",
       " ('edge', 304),\n",
       " ('free', 304),\n",
       " ('cute', 304),\n",
       " ('genius', 304),\n",
       " ('outside', 303),\n",
       " ('reviews', 302),\n",
       " ('admit', 302),\n",
       " ('ok', 302),\n",
       " ('younger', 302),\n",
       " ('fighting', 301),\n",
       " ('odd', 301),\n",
       " ('master', 301),\n",
       " ('recent', 300),\n",
       " ('thanks', 300),\n",
       " ('break', 300),\n",
       " ('comment', 300),\n",
       " ('apart', 299),\n",
       " ('emotions', 298),\n",
       " ('lovely', 298),\n",
       " ('begin', 298),\n",
       " ('doctor', 297),\n",
       " ('party', 297),\n",
       " ('italian', 297),\n",
       " ('la', 296),\n",
       " ('missed', 296),\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the counts of the most common words in positive reviews\n",
    "positive_counts.most_common()\n",
    "# Examine the counts of the most common words in negative reviews\n",
    "negative_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결론에서 볼 수 있듯이, 긍정이든 부정이든 'the', 'a'같은 단어가 매우 자주 등장하는 것을 볼 수 있다. 우리는 가장 많이 등장하는 단어를 찾는 것이 아니라 긍정/부정을 잘 표현하는 단어를 찾고싶다. 따라서 **긍정 리뷰와 부정 리뷰 사이 단어의 비율**을 찾아본다.\n",
    "\n",
    "> **TODO:** 모든 단어들의 positive : negative 비율을 찾아 `pos_neg_ratios`에 저장한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Counter object to store positive/negative ratios\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# TODO: Calculate the ratios of positive and negative uses of the most common words\n",
    "#       Consider words to be \"common\" if they've been used at least 100 times\n",
    "for word in total_counts:\n",
    "    pos_neg_ratios[word] = positive_counts[word] / (float(negative_counts[word])+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 조사한 단어의 몇가지 예를 보자\n",
    "\n",
    "Examine the ratios you've calculated for a few words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 몇가지 규칙이 보인다.\n",
    "\n",
    "> * 긍정적인 느낌의 단어들 (예를 들어 \"amazing\") 은 ratio가 1보다 크다.\n",
    "> 긍정적인 느낌이 더 많이 들고 자주 등장할수록 1에서 멀어진다.\n",
    "> \n",
    "> * 부정적인 느낌의 단어들 (예를 들어 \"terrible\") 은 ratio가 1보다 작다.\n",
    "> 마찬가지로 부정적인 느낌이 더 많이 들고 자주 등장할 수록 0과 가까워진다.\n",
    "> \n",
    "> * 중립적인 느낌의 단어들 (예를 들어 \"the\") 은 ratio가 1과 비슷하다.\n",
    "\n",
    "이제 우리는 ratio를 이용해 감정을 구별할 수 있겠다.\n",
    "하지만 아직은 계산을 하기가 조금 어렵다. 매우 긍적적인 단어 \"amazing\"은 ratio 값이 4이고, 부정적인 단어 \"terrible\"은 값이 0에 가깝기 때문에 몇가지 문제가 발생한다.\n",
    "\n",
    "> * 1은 중립적이다. \"amazing\"은 값이 4이고 \"terrible\"은 값이 0.18인데, 두 단어와 1과의 거리는 몇 배나 차이난다. 따라서 이 값으로 직접 비교는 할 수 없다.\n",
    "> * 따라서 중립적인 값을 중심으로 이러한 불균형을 보정해주는 과정이 필요하다.\n",
    "> * 중간 값은 1보다 0이 계산하기 편하다.\n",
    "\n",
    "우리는 ratio를 구하는데 나누기을 사용했다.\n",
    "이런 종류의 보정에는 log함수가 잘 작동한다.\n",
    "\n",
    "> **TODO:** 모든 ratio값을 log로 변환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert ratios to logs\n",
    "print (pos_neg_ratios[\"iraq\"])\n",
    "\n",
    "for word in pos_neg_ratios:\n",
    "    if pos_neg_ratios[word] != 0:\n",
    "        pos_neg_ratios[word] = np.log(pos_neg_ratios[word])\n",
    "\n",
    "print(pos_neg_ratios[\"iraq\"])\n",
    "#     pos_neg_ratios[word] = np.log(pos_neg_ratios[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 새롭게 정의된 값들을 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 중립적인 단어는 0에 가까운 값을 가지고, 긍정적인 단어 \"Amazing\"과 부정적인 단어 \"terrible\"의 1보다 큰 값을 가지면서 서로 부호가 다른 것을 볼 수 있다. 이는 납득할 수 있는 것이다. 값이 양수이면 긍정적, 음수이면 부정적인 느낌이 강하다고 볼 수 있다.\n",
    "\n",
    "이제 가장 긍정적인 느낌의 단어를 보기위해 값들을 내림차순 정리해주자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words most frequently seen in a review with a \"POSITIVE\" label\n",
    "pos_neg_ratios.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 부정적인 리뷰에 가장 빈번하게 출연했던 단어들을 살펴보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reversed(pos_neg_ratios.most_common()))[0:30]\n",
    "#pos_neg_ratios.most_common()[:-31:-1] 라고 쓸 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면, 중립적인 단어들은 0에 가깝고 긍정적인 리뷰에 더 많이 등장한 단어는 +3 정도의 최댓값을, 부정적인 리뷰에 더 많이 등장한 단어는 -3정도의 최댓값을 가진다. 이것이 우리가 logarithm을 사용한 이유이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "review = \"This was a horrible, terrible movie.\"\n",
    "\n",
    "Image(filename='sentiment_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"The movie was excellent\"\n",
    "\n",
    "Image(filename='sentiment_network_pos.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Creating the Input/Output Data<a id='project_2'></a>\n",
    "\n",
    "**TODO:** 이제 모든 단어를 담고있는 `Vocab`이라는 [set](https://docs.python.org/3/tutorial/datastructures.html#sets) 을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create set named \"vocab\" containing all of the words from all of the reviews\n",
    "vocab = set(total_counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 cell을 실행시키면 vocab의 사이즈를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 이미지를 보자.\n",
    "이제 우리는 아래 이미지와 같은 신경망을 코딩할 것이다.\n",
    "`layer_0`는 input layer, `layer_1`은 hidden_layer, `layer_2`는 output layer에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='sentiment_network_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** `layer_0`에 해당하는 numpy array를 만들고 모두 0으로 초기화한다.이때 `layer_0`는 1개의 row와 `vocab_size`만큼의 columns를 가진 2차원 matrix이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = np.zeros(( 1,vocab_size ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 셀을 실행시키면, `(1, 74074)`가 나와야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='sentiment_network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_0` 는 모든 단어에 대해 한 개의 entry를 가지고 있다.\n",
    "이제 각 단어에 대한 index를 알아야 한다. 따라서 모든 단어에 대한 index를 저장한 lookup table을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of words in the vocabulary mapped to index positions\n",
    "# (to be used in layer_0)\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "# display the map of words to indices\n",
    "word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  `update_input_layer`는 각 단어가 주어진 review에 얼마나 많이 등장한지 그 횟수를 세어서 저장한 layer이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    \"\"\" Modify the global layer_0 to represent the vector form of review.\n",
    "    The element at a given index of layer_0 should represent\n",
    "    how many times the given word occurs in the review.\n",
    "    Args:\n",
    "        review(string) - the string of the review\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    global layer_0\n",
    "    # clear out previous state by resetting the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "        \n",
    "    # TODO: count how many times each word is used in the given review and store the results in layer_0\n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 cell을 실행시켜서 첫번째 review에 대한 input layer를 업데이트 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_input_layer(reviews[0])\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** `get_target_for_labels` 함수를 만든다. 이 함수는 주어진 label이 `NEGATIVE` 이나 `POSITIVE`인지에 따라 `0` 이나 `1`을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_for_label(label):\n",
    "    \"\"\"Convert a label to `0` or `1`.\n",
    "    Args:\n",
    "        label(string) - Either \"POSITIVE\" or \"NEGATIVE\".\n",
    "    Returns:\n",
    "        `0` or `1`.\n",
    "    \"\"\"\n",
    "    if label == \"POSITIVE\":\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'POSITIVE'` label을 입력하면 `1`이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target_for_label(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'NEGATIVE'` label을 입력하면 `0`이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target_for_label(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Building a Neural Network<a id='project_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 이제 `SentimentalNetwork` Class를 만들어보자.\n",
    "- 기본적인 신경망 구조를 코딩한다. 이 구조는 input layer, hidden layer 그리고 output layer로 구성되어있다.\n",
    "- 이때, hidden layer에 비선형성(non-linearity)을 **구현하지 않는다.** 이 말은 hidden layer의 출력에 activation function을 사용하지 않는다는 말이다.\n",
    "- 앞에서 만든 코드들을 활용한다.\n",
    "- `train` 함수는 전체 corpus에 대해서 학습하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Encapsulate our neural network in a class\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Assign a seed to our random number generator to ensure we get\n",
    "        # reproducable results during development \n",
    "        np.random.seed(1)\n",
    "\n",
    "        # process the reviews and their associated labels so that everything\n",
    "        # is ready for training\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        # Build the network to have the number of hidden nodes and the learning rate that\n",
    "        # were passed into this initializer. Make the same number of input nodes as\n",
    "        # there are vocabulary words and create a single output node.\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        # TODO: populate review_vocab with all of the words in the given reviews\n",
    "        #       Remember to split reviews into individual words \n",
    "        #       using \"split(' ')\" instead of \"split()\".\n",
    "        review_vocab = set()\n",
    "        \n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                review_vocab.add(word)\n",
    "        \n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        \n",
    "        # TODO: populate label_vocab with all of the words in the given labels.\n",
    "        #       There is no need to split the labels because each one is a single word.\n",
    "        label_vocab = set()\n",
    "        \n",
    "        for label in labels :\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        # TODO: populate self.word2index with indices for all the words in self.review_vocab\n",
    "        #       like you saw earlier in the notebook\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab) :\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        # TODO: do the same thing you did for self.word2index and self.review_vocab, \n",
    "        #       but for self.label2index and self.label_vocab instead\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab) :\n",
    "            self.label2index[label] = i\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Store the number of nodes in input, hidden, and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        \n",
    "        # TODO: initialize self.weights_0_1 as a matrix of zeros. These are the weights between\n",
    "        #       the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        \n",
    "        # TODO: initialize self.weights_1_2 as a matrix of random values. \n",
    "        #       These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        # TODO: Create the input layer, a two-dimensional matrix with shape \n",
    "        #       1 x input_nodes, with all values initialized to zero\n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "        # TODO: You can copy most of the code you wrote for update_input_layer \n",
    "        #       earlier in this notebook. \n",
    "        #\n",
    "        #       However, MAKE SURE YOU CHANGE ALL VARIABLES TO REFERENCE\n",
    "        #       THE VERSIONS STORED IN THIS OBJECT, NOT THE GLOBAL OBJECTS.\n",
    "        #       For example, replace \"layer_0 *= 0\" with \"self.layer_0 *= 0\"\n",
    "        \n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "        \n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        # TODO: Copy the code you wrote for get_target_for_label \n",
    "        #       earlier in this notebook. \n",
    "        \n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        # TODO: Return the result of calculating the sigmoid activation function\n",
    "        #       shown in the lectures\n",
    "        return (1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        # TODO: Return the derivative of the sigmoid activation function, \n",
    "        #       where \"output\" is the original output from the sigmoid fucntion \n",
    "        return (1 - output)*(output)\n",
    "\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        # Keep track of correct predictions to display accuracy during training \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        # Remember when we started for printing time statistics\n",
    "        start = time.time()\n",
    "\n",
    "        # loop through all the given reviews and run a forward and backward pass,\n",
    "        # updating weights for every item\n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            # TODO: Get the next review and its correct label\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            # TODO: Implement the forward pass through the network. \n",
    "            #       That means use the given review to update the input layer, \n",
    "            #       then calculate values for the hidden layer,\n",
    "            #       and finally calculate the output layer.\n",
    "            # \n",
    "            #       Do not use an activation function for the hidden layer,\n",
    "            #       but use the sigmoid activation function for the output layer.\n",
    "            \n",
    "            #Update Input Layer\n",
    "            self.update_input_layer(review)\n",
    "            \n",
    "            #Layer 1\n",
    "            layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "            \n",
    "            #Layer 2\n",
    "            layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "            \n",
    "            # TODO: Implement the back propagation pass here. \n",
    "            #       That means calculate the error for the forward pass's prediction\n",
    "            #       and update the weights in the network according to their\n",
    "            #       contributions toward the error, as calculated via the\n",
    "            #       gradient descent and back propagation algorithms you \n",
    "            #       learned in class.\n",
    "            \n",
    "            #Output Error\n",
    "            # error = y - hat{y}\n",
    "            # output_delta = error * sigmoid`{y}\n",
    "            layer_2_error = layer_2 - get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "            \n",
    "            #Backpropagate Error\n",
    "            layer_1_error = np.dot(layer_2_delta,self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            #Update Weight\n",
    "            self.weights_0_1 -= np.dot(self.layer_0.T, layer_1_delta) * self.learning_rate\n",
    "            self.weights_1_2 -= np.dot(layer_1.T, layer_2_delta) * self.learning_rate\n",
    "            \n",
    "            \n",
    "            # TODO: Keep track of correct predictions. To determine if the prediction was\n",
    "            #       correct, check that the absolute value of the output error \n",
    "            #       is less than 0.5. If so, add one to the correct_so_far count.\n",
    "            if layer_2 >= 0.5 and label == \"POSITIVE\" : \n",
    "                correct_so_far += 1\n",
    "            elif layer_2 < 0.5 and label == \"NEGATIVE\" :\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the training process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict\n",
    "        # its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the prediction process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "        # TODO: Run a forward pass through the network, like you did in the\n",
    "        #       \"train\" function. That means use the given review to \n",
    "        #       update the input layer, then calculate values for the hidden layer,\n",
    "        #       and finally calculate the output layer.\n",
    "        #\n",
    "        #       Note: The review passed into this function for prediction \n",
    "        #             might come from anywhere, so you should convert it \n",
    "        #             to lower case prior to using it.\n",
    "        self.update_input_layer(review)\n",
    "        layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "        \n",
    "        \n",
    "        # TODO: The output layer should now contain a prediction. \n",
    "        #       Return `POSITIVE` for predictions greater-than-or-equal-to `0.5`, \n",
    "        #       and `NEGATIVE` otherwise.\n",
    "        if layer_2[0] >= 0.5: return \"POSITIVE\"\n",
    "        else: return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `SentimentNetwork`을 만든다.\n",
    "마지막 1,000개의 리뷰는 testing data이다.\n",
    "여기서, `learning rate = 0.1` 로 설정해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 1,000개 리뷰 (test set)에 대해서 테스트를 한번 진행해본다.\n",
    "아직 네트워크를 학습시키지 않았으므로 정확도는 50%가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습을 진행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아마 학습이 잘 진행되지 않을 것이다. 그 이유는 바로 learning rate가 너무 크게 설정되었기 때문이다. 조금 더 작은 값인 `0.01`로 설정하고 다시 학습을 진행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.01)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직도 잘 되지 않을 것이다. 조금 더 작게 해보자. 이번에는 `0.001`로 설정하고 학습을 진행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.001)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate가 `0.001`일때, 비로소 신경망이 유의미한 추정을 하기 시작했다. 아직 썩 맘에 들지는 않지만, 이러한 방법의 가능성은 엿볼 수 있었다. 이제 이 신경망을 더욱 개선하도록 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Neural Noise<a id='lesson_4'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='sentiment_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    \n",
    "    global layer_0\n",
    "    \n",
    "    # clear out previous state, reset the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "\n",
    "update_input_layer(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in reviews[0].split(\" \"):\n",
    "    review_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Reducing Noise in Our Input Data<a id='project_4'></a>\n",
    "\n",
    "신경망은 input data의 질에 따라서 성능이 크게 달라진다. input data는 유의미한 값 뿐만 아니라 각종 noise들도 많이 포함되어 있다. 우리는 이러한 noise를 이해하고 지워줄 것이다.\n",
    "\n",
    "**TODO:** `update_input_layer` 함수를 수정하자. 각 단어별로 '얼마나 많이 있는지' 세지 말고, '있는지' 확인만 하도록 하자. 즉, 단어별로 count를 세지 말고 있으면 0, 없으면 1이 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "+++                self.layer_0[0][self.word2index[word]] += 1\n",
    "---                self.layer_0[0][self.word2index[word]] = 1\n",
    "```\n",
    "딱 `+` 하나만 지워준다.\n",
    "그 결과는 놀라울 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: -Copy the SentimentNetwork class from Projet 3 lesson\n",
    "#       -Modify it to reduce noise, like in the video \n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                review_vocab.add(word)\n",
    "\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        label_vocab = set()\n",
    "        for label in labels :\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab) :\n",
    "            self.word2index[word] = i\n",
    "\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab) :\n",
    "            self.label2index[label] = i\n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Store the number of nodes in input, hidden, and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "        \n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        \n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return (1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return (1 - output)*(output)\n",
    "\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        correct_so_far = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(training_reviews)):\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #Update Input Layer\n",
    "            self.update_input_layer(review)\n",
    "            \n",
    "            #Layer 1\n",
    "            layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "            \n",
    "            #Layer 2\n",
    "            layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "            \n",
    "            #Output Error\n",
    "            layer_2_error = layer_2 - get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "            \n",
    "            #Backpropagate Error\n",
    "            layer_1_error = np.dot(layer_2_delta,self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            #Update Weight\n",
    "            self.weights_0_1 -= np.dot(self.layer_0.T, layer_1_delta) * self.learning_rate\n",
    "            self.weights_1_2 -= np.dot(layer_1.T, layer_2_delta) * self.learning_rate\n",
    "            \n",
    "            if layer_2 >= 0.5 and label == \"POSITIVE\" : \n",
    "                correct_so_far += 1\n",
    "            elif layer_2 < 0.5 and label == \"NEGATIVE\" :\n",
    "                correct_so_far += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "        self.update_input_layer(review)\n",
    "        layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "        \n",
    "        if layer_2[0] >= 0.5: return \"POSITIVE\"\n",
    "        else: return \"NEGATIVE\"\n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "    \n",
    "        \n",
    "    def update_input_layer(self,review):\n",
    "        \n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(\" \"):\n",
    "            if(word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "        \n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "\n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return (1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return (1 - output)*(output)\n",
    "\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        correct_so_far = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(training_reviews)):\n",
    "\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            self.update_input_layer(review)\n",
    "\n",
    "            layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "            layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "            \n",
    "            #Output Error\n",
    "            layer_2_error = layer_2 - get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "            \n",
    "            #Backpropagate Error\n",
    "            layer_1_error = np.dot(layer_2_delta,self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            #Update Weight\n",
    "            self.weights_0_1 -= np.dot(self.layer_0.T, layer_1_delta) * self.learning_rate\n",
    "            self.weights_1_2 -= np.dot(layer_1.T, layer_2_delta) * self.learning_rate\n",
    "            \n",
    "            if layer_2 >= 0.5 and label == \"POSITIVE\" : \n",
    "                correct_so_far += 1\n",
    "            elif layer_2 < 0.5 and label == \"NEGATIVE\" :\n",
    "                correct_so_far += 1\n",
    "\n",
    "            #debug\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "\n",
    "        self.update_input_layer(review)\n",
    "        layer_1 = np.dot(self.layer_0,self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(np.dot(layer_1,self.weights_1_2))\n",
    "        \n",
    "        if layer_2[0] >= 0.5: return \"POSITIVE\"\n",
    "        else: return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 학습을 해보자. \n",
    "`learning rate`는 `0.1`부터 시작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "놀랍다. learning rate가 0.1인데도 불구하고 정확도가 놀라울 정도로 향상됬다.\n",
    "이전에는 *전혀* 학습이 되질 않았던 것에 비하면 정말 놀라운 효과이다.\n",
    "`learning rate`를 `0.001`로 두고 다시 한번 학습을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.001)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~놀랍게도(?)~~ learning rate를 1/100으로 줄였지만 효과는 매우 미미했다.\n",
    "이제 test set에 대해서 test를 진행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Inefficiencies in our Network<a id='lesson_5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sentiment_network_sparse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0[4] = 1\n",
    "layer_0[9] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_1 = np.random.randn(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0.dot(weights_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [4,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indices:\n",
    "    layer_1 += (1 * weights_0_1[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sentiment_network_sparse_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indices:\n",
    "    layer_1 += (weights_0_1[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Making our Network More Efficient<a id='project_5'></a>\n",
    "이전 project에서는 Noise를 제거해서 신경망의 정확도를 높였다.\n",
    "이번 project에서는 불필요한 연산을 줄여 (최적화) 학습속도를 빠르게 만들겠다.\n",
    "\n",
    "**TODO:** \n",
    "* 이제 `update_input_layer` 함수는 필요없다.\n",
    "* `init_network`를 수정한다.:\n",
    ">* 이제 input layer를 분리하지 않을 것이다. 따라서 `self.layer_0`에 관련된 표현은 모두 삭제한다.\n",
    ">* hidden layer는 더욱 직접적으로 다뤄줄 것이다. 따라서 `self.layer_1`를 만든다. 이 layer는 1 x hidden_nodes의 차원을 가지고 있는 2차원 matrix이다. 초기값은 모두 0이다.\n",
    "\n",
    "* `train` 함수를 수정한다.:\n",
    ">* 입력 변수 `training_reviews`를 `training_reviews_raw`로 수정한다.\n",
    ">* 함수의 앞부분에 모든 review들을 indices의 list로 바꿔줄 것이다. (`word2index`를 사용한다.) 그리고 `training_reviews_raw`의 각 review를 할당한 `training_reviews`라는 이름의 local `list`를 만든다. 이 list들은 review의 각 단어에 대한 indice를 담고 있다.\n",
    ">* `update_input_layer` 호출을 삭제한다.\n",
    ">* local `layer_1` 대신 `self`의 `layer_1`을 사용한다.\n",
    ">* forward pass에서 `layer_1`을 업데이트 하는 과정을 삭제한다. input value가 1 아니면 0이기 때문에, 우리는 곱셈의 과정을 생략하고 ( num * 1 = num ), input이 1이면 weight를 더해주고 0이면 지나갈 것이다.\n",
    "* `run` 함수를 수정한다.:\n",
    ">* `update_input_layer` 호출을 삭제한다.\n",
    ">* local `layer_1` 대신 `self`의 `layer_1`을 사용한다.\n",
    ">* `train`에 햇던 것처럼 `review`에 대한 pre-process 과정을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: -Copy the SentimentNetwork class from Project 4 lesson\n",
    "#       -Modify it according to the above instructions \n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(1)\n",
    "        self.pre_process_data(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "\n",
    "        review_vocab = set()\n",
    "        \n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        label_vocab = set()\n",
    "        \n",
    "        for label in labels :\n",
    "            label_vocab.add(label)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab) :\n",
    "            self.word2index[word] = i\n",
    "\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab) :\n",
    "            self.label2index[label] = i\n",
    "            \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n",
    "\n",
    "# 이제 input layer를 분리하지 않을 것이다. 따라서 `self.layer_0`에 관련된 표현은 모두 삭제한다.\n",
    "# hidden layer는 더욱 직접적으로 다뤄줄 것이다. 따라서 `self.layer_1`를 만든다. 이 layer는 1 x hidden_nodes의 차원을 가지고 있는 2차원 matrix이다. 초기값은 모두 0이다.        \n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return (1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return (1 - output)*(output)\n",
    "\n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "# **TODO**:\n",
    "# >* 입력 변수 `training_reviews`를 `training_reviews_raw`로 수정한다.\n",
    "# >* 함수의 앞부분에 모든 review들을 indices의 list로 바꿔줄 것이다. (`word2index`를 사용한다.) 그리고 `training_reviews_raw`의 각 review를 할당한 `training_reviews`라는 이름의 local `list`를 만든다. 이 list들은 review의 각 단어에 대한 indice를 담고 있다.\n",
    "# >* `update_input_layer` 호출을 삭제한다.\n",
    "# >* local `layer_1` 대신 `self`의 `layer_1`을 사용한다.\n",
    "# >* forward pass에서 `layer_1`을 업데이트 하는 과정을 삭제한다. input value가 1 아니면 0이기 때문에, 우리는 곱셈의 과정을 생략하고 ( num * 1 = num ), input이 1이면 weight를 더해주고 0이면 지나갈 것이다.\n",
    "\n",
    "        training_reviews = list()\n",
    "        for review_raw in training_reviews_raw :\n",
    "            review = set()\n",
    "            for word in review_raw.split(\" \"):\n",
    "                if (word in self.word2index.keys()):\n",
    "                    review.add(self.word2index[word])\n",
    "            training_reviews.append(list(review))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        correct_so_far = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(training_reviews)):\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            self.layer_1 *= 0\n",
    "\n",
    "            for j in review:\n",
    "                self.layer_1 += (self.weights_0_1[j])\n",
    "            \n",
    "            layer_2 = self.sigmoid(np.dot(self.layer_1,self.weights_1_2))\n",
    "            \n",
    "            layer_2_error = layer_2 - get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            layer_1_error = np.dot(layer_2_delta,self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "\n",
    "            for j in review:\n",
    "                self.weights_0_1[j] -= layer_1_delta[0] * self.learning_rate\n",
    "            \n",
    "            self.weights_1_2 -= np.dot(self.layer_1.T, layer_2_delta) * self.learning_rate\n",
    "\n",
    "            if layer_2 >= 0.5 and label == \"POSITIVE\" : \n",
    "                correct_so_far += 1\n",
    "            elif layer_2 < 0.5 and label == \"NEGATIVE\" :\n",
    "                correct_so_far += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "\n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "\n",
    "# >* `update_input_layer` 호출을 삭제한다.\n",
    "# >* local `layer_1` 대신 `self`의 `layer_1`을 사용한다.\n",
    "# >* `train`에 햇던 것처럼 `review`에 대한 pre-process 과정을 추가한다.\n",
    "    \n",
    "        \n",
    "        self.layer_1 *= 0\n",
    "        indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if (word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "        \n",
    "        for i in indices:\n",
    "                self.layer_1 += self.weights_0_1[i]   \n",
    "                \n",
    "        layer_2 = self.sigmoid(np.dot(self.layer_1,self.weights_1_2))\n",
    "\n",
    "        if layer_2[0] >= 0.5: return \"POSITIVE\"\n",
    "        else: return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Speed(reviews/sec)`에 주목하자.\n",
    "학습속도를 나타낸 것이다.\n",
    "이전에 `300reviews/sec`이었던 학습속도가 몇가지 코드를 수정하니 무려 `1600reviews/sec`으로 빨라졌다!\n",
    "무려 5배 이상 빨라진 것이다. 놀라운 개선이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sentiment_network_sparse_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words most frequently seen in a review with a \"POSITIVE\" label\n",
    "pos_neg_ratios.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words most frequently seen in a review with a \"NEGATIVE\" label\n",
    "list(reversed(pos_neg_ratios.most_common()))[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = np.histogram(list(map(lambda x:x[1],pos_neg_ratios.most_common())), density=True, bins=100, normed=True)\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"Word Positive/Negative Affinity Distribution\")\n",
    "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_frequency = Counter()\n",
    "\n",
    "for word, cnt in total_counts.most_common():\n",
    "    frequency_frequency[cnt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = np.histogram(list(map(lambda x:x[1],frequency_frequency.most_common())), density=True, bins=100, normed=True)\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"The frequency distribution of the words in our corpus\")\n",
    "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Reducing Noise by Strategically Reducing the Vocabulary<a id='project_6'></a>\n",
    "\n",
    "**TODO:** 이제 통계적인 방법을 도입하여 `SentimentNetwork`의 performance를 더욱 개선할 것이다. \n",
    "단어들 중 중립적인 단어가 너무 많아 정확도와 연산량이 낭비되고 있다. \n",
    "중립적인 감정의 단어들은 cutoff한 후 신경망에 input할 것이다.\n",
    "\n",
    "다음과 같은 과정을 추가하여 구현한다.\n",
    "* 조금 등장한 단어들 (예-50회 이하)은 vocabulary에 모두 넣어준다.\n",
    "* 많이 등장한 단어들 (예-50회 이상)은 positive-negative-ratio가 일정 (cutoff) 이상을 때에만 vocabulary에 넣어준다.\n",
    "\n",
    "* `pre_process_data` 함수를 수정한다.\n",
    ">* `min_count` 과 `polarity_cutoff` 변수를 추가한다.\n",
    ">* 리뷰에 사용된 단어의 positive-to-negative ratios를 계산한다. 다른 점이 있다면, 함수가 아니라 별도의 Class로 만들어준다.\n",
    ">* cutoff에 따라 postive-to-negative ratio가 기준 이상인 단어들만 학습에 사용되게 할 수 있다. 적당한 cutoff를 선택한다.  \n",
    ">* 단어가 `min_count` 번 이상 나오는 경우에만 vocabulary에 추가되도록 한다.\n",
    ">* `polarity_cutoff` 이상의 postive-to-negative를 가지는 경우에만 vocabulary에 추가되도록 한다.\n",
    "* `__init__`함수를 수정한다.:\n",
    ">* `min_count` 와 `polarity_cutoff` 변수를 추가해 `pre_process_data`를 호출할때 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: -Copy the SentimentNetwork class from Project 5 lesson\n",
    "#       -Modify it according to the above instructions \n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(1)\n",
    "       \n",
    "# >* `min_count` 와 `polarity_cutoff` 변수를 추가해 `pre_process_data`를 호출할때 사용한다.\n",
    "        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
    "\n",
    "# >* `min_count` 과 `polarity_cutoff` 변수를 추가한다.\n",
    "# >* 리뷰에 사용된 단어의 positive-to-negative ratios를 계산한다. 다른 점이 있다면, 함수가 아니라 별도의 Class로 만들어준다.\n",
    "\n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)) :\n",
    "            if labels[i] == 'POSITIVE':\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "    # >* cutoff에 따라 postive-to-negative ratio가 기준 이상인 단어들만 학습에 사용되게 할 수 있다. 적당한 cutoff를 선택한다.\n",
    "    # >* 단어가 `min_count` 번 이상 나오는 경우에만 vocabulary에 추가되도록 한다.\n",
    "    # >* `polarity_cutoff` 이상의 postive-to-negative를 가지는 경우에만 vocabulary에 추가되도록 한다.\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for word, counts in list(total_counts.most_common()):\n",
    "            if (counts >= 50):\n",
    "                pos_neg_ratio = positive_counts[word] / float(negative_counts[word]+1)\n",
    "                pos_neg_ratios[word] = pos_neg_ratio\n",
    "\n",
    "        for word, ratio in pos_neg_ratios.most_common():\n",
    "            if ratio > 1:\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = np.log(1 / (ratio + 0.01))\n",
    "        #\n",
    "        ## end New for Project 6\n",
    "        ## ----------------------------------------\n",
    "\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if((pos_neg_ratios[word]>=polarity_cutoff) or (pos_neg_ratios[word]<= -polarity_cutoff)):\n",
    "                        review_vocab.add(word)\n",
    "                else:\n",
    "                    review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "\n",
    "        label_vocab = set()\n",
    "        for label in labels :\n",
    "            label_vocab.add(label)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab) :\n",
    "            self.word2index[word] = i\n",
    "\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab) :\n",
    "            self.label2index[label] = i\n",
    "            \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n",
    "\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return (1/(1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return (1 - output)*(output)\n",
    "\n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "        training_reviews = list()\n",
    "        for review_raw in training_reviews_raw :\n",
    "            review = set()\n",
    "            for word in review_raw.split(\" \"):\n",
    "                if (word in self.word2index.keys()):\n",
    "                    review.add(self.word2index[word])\n",
    "            training_reviews.append(list(review))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        correct_so_far = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(training_reviews)):\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            self.layer_1 *= 0\n",
    "\n",
    "            for j in review:\n",
    "                self.layer_1 += (self.weights_0_1[j])\n",
    "            \n",
    "            layer_2 = self.sigmoid(np.dot(self.layer_1,self.weights_1_2))\n",
    "            \n",
    "            layer_2_error = layer_2 - get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            layer_1_error = np.dot(layer_2_delta,self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "\n",
    "            for j in review:\n",
    "                self.weights_0_1[j] -= layer_1_delta[0] * self.learning_rate\n",
    "            \n",
    "            self.weights_1_2 -= np.dot(self.layer_1.T, layer_2_delta) * self.learning_rate\n",
    "\n",
    "            if layer_2 >= 0.5 and label == \"POSITIVE\" : \n",
    "                correct_so_far += 1\n",
    "            elif layer_2 < 0.5 and label == \"NEGATIVE\" :\n",
    "                correct_so_far += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "\n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "        self.layer_1 *= 0\n",
    "        indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if (word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "        \n",
    "        for i in indices:\n",
    "                self.layer_1 += self.weights_0_1[i]   \n",
    "                \n",
    "        layer_2 = self.sigmoid(np.dot(self.layer_1,self.weights_1_2))\n",
    "\n",
    "        if layer_2[0] >= 0.5: return \"POSITIVE\"\n",
    "        else: return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드를 완성했다면, 네트워크를 훈련시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.05,learning_rate=0.01)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 아래 cell을 실행해 test를 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polarity cutoff를 더욱 크게 해서 다시 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.8,learning_rate=0.01)\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis: What's Going on in the Weights?<a id='lesson_7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_full = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=0,polarity_cutoff=0,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_full.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='sentiment_network_sparse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_words(focus = \"horrible\"):\n",
    "    most_similar = Counter()\n",
    "\n",
    "    for word in mlp_full.word2index.keys():\n",
    "        most_similar[word] = np.dot(mlp_full.weights_0_1[mlp_full.word2index[word]],mlp_full.weights_0_1[mlp_full.word2index[focus]])\n",
    "    \n",
    "    return most_similar.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_words(\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_words(\"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "words_to_visualize = list()\n",
    "for word, ratio in pos_neg_ratios.most_common(500):\n",
    "    if(word in mlp_full.word2index.keys()):\n",
    "        words_to_visualize.append(word)\n",
    "    \n",
    "for word, ratio in list(reversed(pos_neg_ratios.most_common()))[0:500]:\n",
    "    if(word in mlp_full.word2index.keys()):\n",
    "        words_to_visualize.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "colors_list = list()\n",
    "vectors_list = list()\n",
    "for word in words_to_visualize:\n",
    "    if word in pos_neg_ratios.keys():\n",
    "        vectors_list.append(mlp_full.weights_0_1[mlp_full.word2index[word]])\n",
    "        if(pos_neg_ratios[word] > 0):\n",
    "            pos+=1\n",
    "            colors_list.append(\"#00ff00\")\n",
    "        else:\n",
    "            neg+=1\n",
    "            colors_list.append(\"#000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "words_top_ted_tsne = tsne.fit_transform(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"vector T-SNE for most polarized words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],\n",
    "                                    x2=words_top_ted_tsne[:,1],\n",
    "                                    names=words_to_visualize,\n",
    "                                    color=colors_list))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source, fill_color=\"color\")\n",
    "\n",
    "word_labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(word_labels)\n",
    "\n",
    "show(p)\n",
    "\n",
    "# green indicates positive words, black indicates negative words"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
